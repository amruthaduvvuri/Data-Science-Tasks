# ML---practise-projects-

# ğŸ’» Laptop Price Prediction â€“ Model Improvement & Regularization

This project focuses on improving a laptop price prediction model using **cross-validation**, **overfitting analysis**, **polynomial regression**, **ridge regression**, and **grid search hyperparameter tuning**.  
The dataset contains various laptop specifications, and the target variable is **Price**.

---

## ğŸ“Š Dataset Overview

The dataset includes the following attributes:

- CPU_frequency  
- RAM_GB  
- Storage_GB_SSD  
- CPU_core  
- OS  
- GPU  
- Category  
- **Price** (Target Variable)

---

## ğŸ¯ Project Objectives

1. Improve model performance using **cross-validation**
2. Identify and analyze **overfitting**
3. Apply **Ridge Regression** to reduce model variance
4. Optimize model performance using **GridSearchCV**

---

## ğŸ§© Task Breakdown

---

### âœ… Task 1: Cross-Validation for Model Improvement

- The dataset is divided into:
  - **x_data** â†’ All independent features
  - **y_data** â†’ Target variable (`Price`)
- Cross-validation is applied to improve model robustness and reduce bias caused by a single train-test split.

**Outcome:**  
More stable and reliable performance evaluation across multiple folds.

---

### âš ï¸ Task 2: Overfitting Analysis using Polynomial Regression

- The dataset is split into:
  - **50% Training Data**
  - **50% Testing Data**
- A polynomial regression model is built using only the feature:
  - `CPU_frequency`
- Polynomial degrees from **1 to 5** are evaluated.
- The **RÂ² score** is calculated for each degree to identify overfitting.

**Key Insight:**  
- Lower-degree polynomials may underfit  
- Higher-degree polynomials may overfit  
- RÂ² scores help identify the optimal complexity level

ğŸ“Œ The RÂ² scores for degrees 1â€“5 are stored in a list for comparison.

---

### ğŸ”’ Task 3: Ridge Regression with Polynomial Features

- Multiple features are used:
  - `CPU_frequency`
  - `RAM_GB`
  - `Storage_GB_SSD`
  - `CPU_core`
  - `OS`
  - `GPU`
  - `Category`
- Polynomial features of **degree = 2** are generated.
- The dataset is split into training and testing sets.
- **Ridge Regression** is applied to control overfitting by penalizing large coefficients.

**Outcome:**  
Improved generalization performance on unseen data.

---

### ğŸ” Task 4: Hyperparameter Tuning using Grid Search

- **GridSearchCV** is used to identify the optimal value of **alpha** for Ridge Regression.
- The same set of features from Task 3 is used.
- Multiple alpha values are tested to find the best regularization strength.

**Outcome:**  
Best-performing model configuration with optimized biasâ€“variance tradeoff.

---

## ğŸ› ï¸ Technologies Used

- Python
- Pandas
- NumPy
- Scikit-learn
- Matplotlib (for analysis and visualization)

---

## ğŸ“ˆ Key Learnings

- Cross-validation improves model reliability
- Polynomial models can easily overfit without regularization
- Ridge Regression helps control variance in complex models
- Grid search ensures optimal hyperparameter selection

--

## ğŸ  House Price Prediction using Support Vector Regression (SVR)

### ğŸ“Œ Overview

This is a **practice machine learning project** aimed at predicting house prices using a **Support Vector Regression (SVR)** model.  
The project uses the **California Housing dataset** from `sklearn.datasets` and explores how factors such as house size, number of rooms, location, median income, and population influence housing prices.

---

### ğŸ“Š Dataset

The dataset is sourced from **sklearn.datasets (California Housing)** and contains the following features:

- **MedInc** â€“ Median income of residents in the area  
- **HouseAge** â€“ Average age of houses in the area  
- **AveRooms** â€“ Average number of rooms per household  
- **AveBedrms** â€“ Average number of bedrooms per household  
- **Population** â€“ Population of the area  
- **AveOccup** â€“ Average number of occupants per household  
- **Longitude** â€“ Geographic longitude  
- **Latitude** â€“ Geographic latitude  

ğŸ¯ **Target Variable:**  
- **House Price** (Median house value)

---

### ğŸ¤– Model Used

- **Support Vector Regression (SVR)**
- Feature scaling applied to ensure optimal performance
- Hyperparameter tuning to improve prediction accuracy

---

### âš™ï¸ Project Workflow

1. Load the California Housing dataset  
2. Perform data preprocessing  
   - Feature scaling  
   - Train-test split  
3. Train the SVR model  
4. Evaluate model performance using:
   - **Mean Squared Error (MSE)**
   - **Mean Absolute Error (MAE)**

---

### ğŸ“ˆ Evaluation Metrics

- **Mean Squared Error (MSE):** Measures average squared difference between predicted and actual prices  
- **Mean Absolute Error (MAE):** Measures average absolute difference between predicted and actual prices  

Lower values indicate better model performance.

---

### ğŸ› ï¸ Tools & Libraries

- Python  
- NumPy  
- Pandas  
- Scikit-learn  

---

### ğŸš€ Future Enhancements

- Compare SVR with Linear Regression and Tree-based models  
- Perform GridSearchCV for advanced hyperparameter tuning  
- Add data visualization for feature impact analysis  
- Deploy the model using a web interface or API

---
